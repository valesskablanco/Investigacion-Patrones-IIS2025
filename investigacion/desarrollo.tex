\section{Desarrollo}
\label{sec:intro}

Para el desarrollo del caso práctico se decidió elaborar un autoencoder con sobre el conjunto de datos de MNIST. Para iniciar se importaron las librerías de PyTorch, Matplotlib y NumPy. Específicamente, PyTorch fue la biblioteca central: se usó \texttt{torch} para las operaciones de tensores y la configuración del dispositivo; \texttt{torch.nn} para construir la clase, sus capas y la función de pérdida, además \texttt{torch.optim} para el optimizador. Para la gestión de los datos, se importaron \texttt{DataLoader} de PyTorch y los módulos \texttt{datasets} y \texttt{transforms} de Torchvision para cargar y pre-procesar el dataset MNIST. Complementariamente, se incluyó \texttt{matplotlib.pyplot} para todas las visualizaciones y \texttt{numpy} para las operaciones numéricas de preparación de datos para el análisis del espacio latente.

A continuación, se definieron los hiperparámetros cruciales para el entrenamiento. Estos valores controlan el proceso de aprendizaje: el \texttt{batch\_size} (tamaño de lote) se fijó en 128, indicando cuántas imágenes se procesan juntas antes de actualizar el modelo; la \texttt{learning\_rate} (tasa de aprendizaje) se estableció en 0.001 ($1e-3$), controlando el tamaño del paso que da el optimizador Adam para ajustar los pesos de la red; el \texttt{num\_epochs} se fijó en 20, determinando cuántas veces el modelo vería el conjunto de datos de entrenamiento completo; y finalmente, el \texttt{latent\_dim} (dimensión latente) se estableció en 32, definiendo el tamaño del "cuello de botella", que es la representación comprimida de la imagen.

En el paso siguiente, se procedió a la preparación de los datos. Se utilizó la biblioteca \texttt{torchvision.datasets} para descargar y cargar automáticamente el dataset MNIST, el cual ya venía previamente definido con su división estándar de 60,000 imágenes para el entrenamiento (\texttt{train=True}) y 10,000 imágenes para las pruebas (\texttt{train=False}). Esta partición dedica aproximadamente el 85.7\% del conjunto al entrenamiento y el 14.3\% restante al testeo. Fue crucial aplicar transformaciones a cada imagen: primero se convirtieron a tensores de PyTorch y luego se normalizaron con media 0.5 y desviación 0.5 para adecuar los valores de los píxeles. Finalmente, estos datasets se envolvieron en \texttt{DataLoaders} para suministrar los datos al modelo de forma eficiente en lotes de 128 imágenes.

Posteriormente, se definió la arquitectura mediante la clase \texttt{Autoencoder}. Esta clase encapsula dos componentes principales: el \texttt{Encoder} (codificador), una red secuencial que reduce la dimensionalidad de la imagen de entrada (784 píxeles) al espacio latente de 32 dimensiones; y el \texttt{Decoder} (decodificador), que revierte este proceso intentando reconstruir la imagen original a partir de esa representación compacta. Sumado a esto, se implementó la función \texttt{forward}, esta función recibe el tensor de entrada (\texttt{x}), lo pasa secuencialmente a través del \texttt{encoder} para obtener el vector latente y luego pasa ese vector latente por el \texttt{decoder} para generar la imagen reconstruida, la cual se retorna. Además, se configuraron los componentes del entrenamiento: se seleccionó el Error Cuadrático Medio como la función de pérdida para medir la diferencia entre la imagen original y la reconstruida, y se instanció el optimizador Adam para manejar la actualización de los pesos del modelo.

Para finalizar, se entrenó el modelo del autoencoder durante 20 épocas. En cada época, primero se ajustaban los pesos del modelo usando los datos de entrenamiento, calculando la pérdida (error) entre las imágenes originales y las reconstruidas mediante MSE y aplicando retropropagación. Inmediatamente después, se evaluaba el rendimiento del modelo con los datos de prueba, calculando la pérdida de la misma maner, y se guardaban los resultados de ambas fases.

Al finalizar el entrenamiento, se graficaron las curvas de pérdida de entrenamiento y prueba para observar la evolución del aprendizaje. Luego, se visualizó el desempeño del modelo tomando un lote de imágenes de prueba: se mostraron las imágenes originales en una fila y, justo debajo, sus correspondientes imágenes reconstruidas por el autoencoder. Finalmente, se utilizó la técnica t-SNE para comprimir los vectores latentes (la salida del \texttt{encoder}) de todo el conjunto de prueba a solo dos dimensiones, creando un gráfico de dispersión coloreado por dígito para visualizar cómo el modelo agrupó las imágenes.

% Contenido de la Introducción
\section{Introducción}
\label{sec:intro}

El aprendizaje automático se ha consolidado como una herramienta fundamental en la extracción de conocimiento a partir de grandes volúmenes de datos. Tradicionalmente, muchos de los métodos más conocidos se enmarcan en el aprendizaje supervisado, donde un algoritmo aprende a partir de un conjunto de datos previamente etiquetado, es decir, donde cada ejemplo de entrada está asociado con una salida o respuesta correcta. Sin embargo, en numerosos escenarios del mundo real, la obtención de estas etiquetas es un proceso costoso, lento o, en ocasiones, simplemente imposible. Es en este contexto donde el aprendizaje no supervisado adquiere una relevancia crítica \cite{Bishop2006}

El aprendizaje no supervisado (Unsupervised Learning) aborda el desafío de encontrar patrones, estructuras o conocimiento inherente en conjuntos de datos que carecen por completo de etiquetas. A diferencia de su contraparte supervisada, el objetivo no es predecir una salida específica, sino más bien explorar la estructura intrínseca de los datos. Los algoritmos de este paradigma actúan como exploradores, buscando afinidades, anomalías o representaciones simplificadas sin ninguna guía externa sobre lo que constituye un resultado "correcto". \cite{Goodfellow2016}

En esta investigación, la técnica de aprendizaje no supervisado implementada es el Autoencoder (AE). Este modelo es un tipo específico de red neuronal artificial diseñado para aprender representaciones eficientes de los datos (codificaciones) sin necesidad de etiquetas \cite{Hinton2006}. Su arquitectura se compone fundamentalmente de dos sub-redes: un \texttt{Encoder} (codificador), que transforma los datos de entrada de alta dimensión en una representación compacta de baja dimensión; y un \texttt{Decoder} (decodificador), que realiza la tarea inversa, intentando reconstruir la entrada original con la mayor fidelidad posible a partir de esa representación comprimida \cite{Goodfellow2016}. El punto central del modelo es el espacio latente (el "cuello de botella"), que es esta representación de baja dimensionalidad. Al entrenar la red para minimizar el "error de reconstrucción" —la diferencia entre la entrada y la salida—, el \texttt{Encoder} es forzado a aprender una codificación que captura únicamente las características más esenciales y la estructura subyacente de los datos, descartando la información redundante o el ruido.
